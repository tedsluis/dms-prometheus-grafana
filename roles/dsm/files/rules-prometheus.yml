groups:
    - name: prometheus.rules
      rules:
      - alert: prometheus-down
        annotations:
          description: 'prometheus is down {{ $labels.instance }}'
          summary: 'prometheus is down'
        expr: up{job="prometheus"} != 1 or absent(up{job="prometheus"}) ==1
        for: 1m
        labels:
          severity: critical
      - alert: prometheus-job-missing
        annotations:
          summery: Prometheus job missing
          description: A Prometheus job has disappeared
        for: 1m
        labels:
          severity: warning
        expr: 'absent(up{job="prometheus"})'
      - alert: prometheus-all-target-missing
        annotations:
          summery: Prometheus all targets missing
          description: A Prometheus job does not have living target anymore.
        for: 1m
        labels:
          severity: critical
        expr: 'count by (job) (up) == 0'
      - alert: prometheus-config-reload-failure
        annotations:
          summery: Prometheus configuration reload failure
          description: Prometheus configuration reload error
        for: 1m
        labels:
          severity: warning
        expr: 'prometheus_config_last_reload_successful != 1'
      - alert: prometheus-too-many-restarts
        annotations:
          summery: Prometheus too many restarts
          description: Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.
        for: 1m
        labels:
          severity: warning
        expr: 'changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2'
      - alert: prometheus-alertmanager-config-reload-failer
        annotations:
          summery: Prometheus AlertManager configuration reload failure
          description: AlertManager configuration reload error
        for: 1m
        labels:
          severity: warning
        expr: 'alertmanager_config_last_reload_successful != 1'
      - alert: prometheus-alertmanager-config-not-syned
        annotations:
          summery: Prometheus AlertManager config not synced
          description: Configurations of AlertManager cluster instances are out of sync
        for: 1m
        labels:
          severity: warning
        expr: 'count(count_values("config_hash", alertmanager_config_hash)) > 1'
      - alert: prometheus-alertmanager-e2e-dead-man-switch
        annotations:
          summery: Prometheus AlertManager E2E dead man switch
          description: Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager.
        for: 1m
        labels:
          severity: critical
        expr: 'vector(1)'
      - alert: prometheus--not-connected-to-alertmanager
        annotations:
          summery: Prometheus not connected to alertmanager
          description: Prometheus cannot connect the alertmanager
        for: 1m
        labels:
          severity: critical
        expr: 'prometheus_notifications_alertmanagers_discovered < 1'
      - alert: prometheus-rule-evaluation-failures
        annotations:
          summery: Prometheus rule evaluation failures
          description: 'Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.'
        for: 1m
        labels:
          severity: critical
        expr: 'increase(prometheus_rule_evaluation_failures_total[3m]) > 0'
      - alert: prometheus-template-text-expension-failures
        annotations:
          summery: Prometheus template text expansion failures
          description: 'Prometheus encountered {{ $value }} template text expansion failures'
        for: 1m
        labels:
          severity: critical
        expr: 'increase(prometheus_template_text_expansion_failures_total[3m]) > 0'
      - alert: prometheus-rule-evaluation-slow
        annotations:
          summery: Prometheus rule evaluation slow
          description: 'Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query.'
        labels:
          severity: warning
        expr: 'prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds'
        for: 5m
      - alert: prometheus-notification-backlog
        annotations:
          summery: Prometheus notifications backlog
          description: The Prometheus notification queue has not been empty for 10 minutes
        for: 1m
        labels:
          severity: warning
        expr: 'min_over_time(prometheus_notifications_queue_length[10m]) > 0'
      - alert: prometheus-alertmanager-notification-failing
        annotations:
          summery: Prometheus AlertManager notification failing
          description: Alertmanager is failing sending notifications
        for: 1m
        labels:
          severity: critical
        expr: 'rate(alertmanager_notifications_failed_total[1m]) > 0'
      - alert: prometheus-target-empty
        annotations:
          summery: Prometheus target empty
          description: Prometheus has no target in service discovery
        for: 1m
        labels:
          severity: critical
        expr: 'prometheus_sd_discovered_targets == 0'
      - alert: prometheus-target-scraping-slow
        annotations:
          summery: Prometheus target scraping slow
          description: Prometheus is scraping exporters slowly
        labels:
          severity: warning
        expr: 'prometheus_target_interval_length_seconds{quantile="0.9"} > 60'
        for: 5m
      - alert: prometheus-large-scrape
        annotations:
          summery: Prometheus large scrape
          description: Prometheus has many scrapes that exceed the sample limit
        labels:
          severity: warning
        expr: 'increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10'
        for: 5m
      - alert: prometheus-targetscrape-duplicate
        annotations:
          summery: Prometheus target scrape duplicate
          description: Prometheus has many samples rejected due to duplicate timestamps but different values
        for: 1m
        labels:
          severity: warning
        expr: 'increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0'
      - alert: prometheus-tsdb-checkpoint-creation-failures
        annotations:
          summery: Prometheus TSDB checkpoint creation failures
          description: 'Prometheus encountered {{ $value }} checkpoint creation failures'
        for: 1m
        labels:
          severity: critical
        expr: 'increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0'
      - alert: prometheus-tsdb-checkpoint-deletion-failures
        annotations:
          summery: Prometheus TSDB checkpoint deletion failures
          description: 'Prometheus encountered {{ $value }} checkpoint deletion failures'
        for: 1m
        labels:
          severity: critical
        expr: 'increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0'
      - alert: prometheus-tsdb-compactions-failed
        annotations:
          summery: Prometheus TSDB compactions failed
          description: 'Prometheus encountered {{ $value }} TSDB compactions failures'
        for: 1m
        labels:
          severity: critical
        expr: 'increase(prometheus_tsdb_compactions_failed_total[1m]) > 0'
      - alert: prometheus-tsdb-head-truncations-failed
        annotations:
          summery: Prometheus TSDB head truncations failed
          description: 'Prometheus encountered {{ $value }} TSDB head truncation failures'
        for: 1m
        labels:
          severity: critical
        expr: 'increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0'
      - alert: prometheus-tsdb-relad-failures
        annotations:
          summery: Prometheus TSDB reload failures
          description: 'Prometheus encountered {{ $value }} TSDB reload failures'
        for: 1m
        labels:
          severity: critical
        expr: 'increase(prometheus_tsdb_reloads_failures_total[1m]) > 0'
      - alert: prometheus-tsdb-wal-corruptions
        annotations:
          summery: Prometheus TSDB WAL corruptions
          description: 'Prometheus encountered {{ $value }} TSDB WAL corruptions'
        for: 1m
        labels:
          severity: critical
        expr: 'increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0'
      - alert: prometheus-tsdb-wal-truncations-failed
        annotations:
          summery: Prometheus TSDB WAL truncations failed
          description: 'Prometheus encountered {{ $value }} TSDB WAL truncation failures'
        for: 1m
        labels:
          severity: critical
        expr: 'increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0'
